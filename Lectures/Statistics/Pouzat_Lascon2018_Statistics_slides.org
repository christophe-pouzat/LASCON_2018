# -*- ispell-local-dictionary: "american" -*-
#+TITLE: A glimpse of the Statistician's toolbox
#+AUTHOR: @@latex:{\large Christophe Pouzat} \\ \vspace{0.2cm}MAP5, Paris-Descartes University and CNRS\\ \vspace{0.2cm} \texttt{christophe.pouzat@parisdescartes.fr}@@
#+DATE: LASCON, January 22 2018, Lecture 2
#+OPTIONS: H:2 tags:nil
#+EXCLUDE_TAGS: noexport
#+LANGUAGE: en
#+SELECT_TAGS: export
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [presentation]
#+LATEX_HEADER: \usepackage{dsfont}
#+BEAMER_HEADER: \setbeamercovered{invisible}
#+BEAMER_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{Where are we ?}\tableofcontents[currentsection]\end{frame}}
#+BEAMER_HEADER: \beamertemplatenavigationsymbolsempty
#+STARTUP: beamer
#+COLUMNS: %45ITEM %10BEAMER_ENV(Env) %10BEAMER_ACT(Act) %4BEAMER_COL(Col) %8BEAMER_OPT(Opt)
#+STARTUP: indent
#+PROPERTY: header-args :eval no-export

* Setup :noexport:
#+NAME: set-gnuplot-pars
#+BEGIN_SRC gnuplot :session *gnuplot* :results silent :eval no-export :exports none 
set terminal pngcairo size 1000,1000
#+END_SRC

#+NAME: stderr-redirection
#+BEGIN_SRC emacs-lisp :exports none
;; Redirect stderr output to stdout so that it gets printed correctly (found on
;; http://kitchingroup.cheme.cmu.edu/blog/2015/01/04/Redirecting-stderr-in-org-mode-shell-blocks/
(setq org-babel-default-header-args:sh
      '((:prologue . "exec 2>&1") (:epilogue . ":"))
      )
(setq org-babel-use-quick-and-dirty-noweb-expansion t)
#+END_SRC

#+RESULTS: stderr-redirection
: t

* Introduction :export:
** What are we going to talk about? 
- Descriptive statistics that are *robust*: =median=, =median absolute deviation=, =five-number summary=.
- =Cumulative Distribution Functions= (CDF) and their observed or empirical versions (ECDF).
- The =Likelihood= function.
- The =Maximum Likelihood Estimator= (MLE) and its properties.

* Descriptive statistics :export:
** What makes a statistic "robust"?
- Robust statistics are "well behaved" even when something goes wrong.
- We will illustrate what that means with robust versions of the classical /location/ and /scale/ parameters:
  + the *median* instead of the  /mean/,
  + the *median absolute deviation* (MAD) instead of the /standard deviation/.  

** An example
24 determinations of the copper content in the wholemeal floor (in parts per million) sorted in ascending order (example 1.1 of Maronna, Martin & Yohai, /Robust Statistics. Theory and Methods./ 2006 J. Wiley):
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{tabular}{ r r r r r r r r } 
2.20 & 2.20 & 2.40 & 2.40 & 2.50 & 2.70 & 2.80 & 2.90\\
3.03 & 3.03 & 3.10 & 3.37 & 3.40 & 3.40 & 3.40 & 3.50\\
3.60 & 3.70 & 3.70 & 3.70 & 3.70 & 3.77 & 5.28 & \textcolor{orange}{28.95}
\end{tabular}
#+END_EXPORT

#+BEGIN_EXPORT latex
\vspace{0.2cm}
#+END_EXPORT

- The sample /mean/ is 4.28 while the /median/ is 3.38.

#+BEGIN_EXPORT latex
\vspace{0.2cm}
#+END_EXPORT
If we remove the *outlier*, 28.95, we get:
#+BEGIN_EXPORT latex
\vspace{0.2cm}
#+END_EXPORT
- A /mean/ of 3.21 and a /median/ of 3.37.

*** Making the graph 
:PROPERTIES:
:BEAMER_ENV: note
:END:
#+NAME: copper-table
|  2.20 | 0 |
|  2.20 | 1 |
|  2.40 | 0 |
|  2.40 | 1 |
|  2.50 | 0 |
|  2.70 | 0 |
|  2.80 | 0 |
|  2.90 | 0 |
|  3.03 | 0 |
|  3.03 | 1 |
|  3.10 | 0 |
|  3.37 | 0 |
|  3.40 | 0 |
|  3.40 | 1 |
|  3.40 | 2 |
|  3.50 | 0 |
|  3.60 | 0 |
|  3.70 | 0 |
|  3.70 | 1 |
|  3.70 | 2 |
|  3.70 | 3 |
|  3.77 | 0 |
|  5.28 | 0 |
| 28.95 | 0 |

#+NAME: copper-fig
#+HEADERS: :file imgs/copper_fig.png 
#+BEGIN_SRC gnuplot :var data=copper-table :cache no
unset key
unset border
unset label
set zeroaxis lt -1
set ytics (-1,1)
set arrow from 4.28,-0.04 to 4.28,0.0 lc 'red' lw 2
set arrow from 3.21,0.04 to 3.21,0.0 lc 'red' lw 2
set arrow from 3.38,-0.04 to 3.38,0.0 lc 'blue' lw 2
set arrow from 3.37,0.04 to 3.37,0.0 lc 'blue' lw 2
plot [2:7] [-0.05:0.2] data using 1:(0.05*$2) pt 6 ps 2 lc 'black'
#+END_SRC

#+RESULTS: copper-fig
[[file:imgs/copper_fig.png]]


** 
#+ATTR_LATEX: :width 0.8\textwidth
[[./imgs/copper_fig.png]]

#+BEGIN_EXPORT latex
\vspace{0.2cm}
#+END_EXPORT
The data with the outlier out of scale. Bottom: mean (red) and median (blue) computed with the outlier. Top: mean (red) and median (blue) computed /without/ the outlier.

** What is the MAD?

- We have just seen that using a median instead of a mean "stabilizes" the results (understand: make the result look the same when some observations are removed).
- We want to adapt this idea to the statistics characterizing the /scale/ or /spread/ of the data for which the /standard deviation/ (SD) is usually used.
- The SD is moreover obtained from the square root of a mean of *squared differences* (the difference between each individual observation and the sample mean). If one observation is a genuine outlier /it will dominate the estimate/.
** 
- The /Median Absolute Deviation/ addresses both issues.
- It is proportional to the median of the absolute deviations with respect to the median:\[\mathtt{MAD} = \frac{1}{0.67449} \, \mathtt{median}\left(|X_i - \mathtt{median}(X)|\right)\, ,\] where $X=\{X_1,\ldots,X_n\}$ is the sample.
- The division by 0.67449 makes the MAD equal to the SD (on average) when the sample is drawn from a Gaussian.
- For the copper data, the SD is 5.30 with the complete sample and becomes 0.69 when the outlier is removed.
- *For the same sample, the MAD is 0.53 with the complete sample and becomes 0.50 when the outlier is removed*.
 
** 
- *When you work with real data use the median instead of the mean and the MAD instead of the SD* unless you are pretty sure that your sample contains no "pathological" observations.
- We will see that at work on neurophysiological data when we will discuss spike sorting.

** The five-numbers summary
This is a set of statistics that turns out to be very useful to summarize large data set. It is:
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT
- The /minimum/ of the sample.
- The /first quartile/.
- The /median/ (second /quartile/).
- The /third quartile/
- The /maximum/ of the sample.
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT
The /inter quartile range/ (IQR), the difference between the third and first quartile is another robust estimator of the spread of the data.
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT

*When working with large datasets my recommendation is to compute systematically the five-numbers summary and the MAD. These statistics should appear in your lab-book.* 

** The Empirical Cumulative Distribution Function (ECDF) 
- The /Cumulative Distribution Function/ (CDF) of a random variable $X$ is by definition:\[F_X(x) \equiv \mathbb{P}(X \le x) \, ,\] where $\mathbb{P}(X \le x)$ stands for "the probability of the event $X \le x$".  
- Let $X_1,\ldots,X_n \stackrel{\mathrm{IID}}{\sim} F_X$, the /Empirical Cumulative Distribution Function/ (ECDF) of the sample $\{X_1,\ldots,X_n\}$ is (by definition): \[\widehat{\mathrm{F}} \equiv \frac{1}{n} \sum_{i=1}^n \mathds{1}(X_i \le x) \, ,\] where \[\mathds{1}(X_i \le x) = \left\{ \begin{array}{lr} 1 & \mathrm{if}\quad X_i \le x\\ 0 & \mathrm{if}\quad X_i > x.\end{array} \right. \] 

** 
- The ECDF is a function that makes a "jump" of size 1/n at each observation $X_i$.
- If we write $X_{(i)}$ the /order statistics/, that is:
  + $X_{(1)} = \min \{X_1,\ldots,X_n\}$
  + $X_{(2)} = \min \{X_1,\ldots,X_n\} \setminus \{X_{(1)}\}$ 
  + $X_{(k)} = \min \{X_1,\ldots,X_n\} \setminus \{X_{(1)},\ldots,X_{(k-1)}\}$ 
  + $X_{(n)} = \max \{X_1,\ldots,X_n\}$
  the ECDF graph is piecewise constant, continuous on the right side with a limit of the left side (the function's value at a jump site, $X_{(k)}$, is the staircase's height on the right side of $X_{(k)}$).

** An example with historical data 

#+ATTR_LATEX: :width 0.8\textwidth
[[./imgs/FattKatz1952.png]]

In 1952, Fatt and Katz reported the first observation at the frog neuro-muscular junction of *miniature end-plate potentials* (mEPPs).

**  
#+ATTR_LATEX: :width 1.0\textwidth
[[./imgs/FattKatz1952Fig2.png]]

These observations quickly lead to the *quantal release* model of synaptic transmission. Katz got the Nobel Prize for that (Paul Fatt was forgotten...).

** The data

- In their 1952 paper, Fatt and Katz study the time intervals between two successive mEPPs (the results are shown on their Figs. 11 & 12).
- The data can be found in the appendix of a book by David Cox and Peter Lewis (1966) /The Statistical Analysis of Series of Events/ who thank Katz and Miledi for providing the data and who wrongly describe them as measurements between /nerve impulses/ (this is at least what I concluded when I tried to figure out how intervals between nerve impulses--from a single axon--could follow so perfectly a Poisson distribution).
- The data reappear in two recent and excellent books by Larry Wasserman (/All of Statistics/, 2004 and /All of Nonparametric Statistics/, 2006).
- They can be downloaded from L. Wasserman website: [[http://www.stat.cmu.edu/~larry/all-of-nonpar/]].

** 

#+ATTR_LATEX: :width 0.75\textwidth
[[./imgs/Fatt_Katz_ECDF.png]]

ECDF of the inter MEPP times from Fatt and Katz (1952).

** Adding confidence bands

- It is possible and even straightforward to add a *confidence band* to the graph of $\widehat{\mathrm{F}}$.
- A *confidence band* is a domain that contains the *complete graph of the true CDF* with a probability set by us.
- The Kolmogorov distribution can be used, but a simpler to compute distribution--leading to almost as tight bands--results from the *Dvoretzky-Kiefer-Wolfowitz* (DKW) inequality:\[\mathbb{P}\left(\sup_x \mid \mathrm{F}(x) - \widehat{\mathrm{F}}_{n}(x) \mid > \epsilon \right) \le 2\, e^{-2 n \epsilon^2} \, ,\] where $n$ is the sample size.
- So if we want $\epsilon$ such that:\[\mathbb{P}\left(\sup_x \mid \mathrm{F}(x) - \widehat{\mathrm{F}}_{n}(x) \mid > \epsilon \right) \le 1-\alpha\] we find:\[\epsilon(\alpha) = \sqrt{\frac{1}{2 n}} \sqrt{\log\left(\frac{2}{1-\alpha}\right)} \, .\]
 

** 
#+ATTR_LATEX: :width 0.75\textwidth
[[./imgs/Fatt_Katz_ECDF_band.png]]

ECDF with 95% confidence band of the inter MEPP times from Fatt and Katz (1952).

** Why use the ECDF?
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT
- The only "data manipulation" involved is sorting (no bin width setting).
- It is easy to get confidence bands making the ECDF a quantitative tool.
- Histograms lead too easily to baseless conclusions. 

* Models and estimators                                              :export:

** The setting
- We consider a situation where a sample or a set of observations $\mathbold{x}=(x_i)_{i=1,\ldots,n}$ are available.
- These observations are modeled as a draw from a probability distribution $\mathcal{M}$, we say that the sample $\mathbold{x}$ is the *realization* of the random variable $\mathbold{X}$ whose distribution is $\mathcal{M}$ ($\mathbold{X} \sim \mathcal{M}$). 
- Our model $\mathcal{M}$ is in fact partly unknown, otherwise we would not need the experiment that gave us $\mathbold{x}$.
- So we really have in mind a collection of models that we write $\mathcal{M}(\theta)$, where $\theta \in \mathbb{R}^p$ and $p < \infty$.

** 
As an example, we could assume:
- The data were generated by measurements along a decaying mono-exponential that we will call "our signal", $s$: \[s(t;b, \Delta, \tau) \equiv b+\Delta \, \exp -t/\tau \, ,\] where $b$ is the baseline, $\Delta$, the jump at zero and $\tau$ the decay time constant.
- These three quantities constitute our model parameter: $\theta \equiv (b, \Delta, \tau)$. 
- The measurements were done at some specific (positive) times $(t_i)_{i=1,\ldots,n}$.
- The measurements were corrupted by an independent Gaussian noise with a know variance $\sigma^2$ and a null mean, leading to the following expression for the /probability density/: \[p(X_i=x; t_i, \theta) = \frac{1}{\sqrt{2\,\pi\,\sigma^2}} \, \exp \left(- \frac{(x-s(t_i;\theta))^2}{2 \sigma^2} \right)\, .\]

** 
- Since we assume that the measurement noise is independent of the signal value and of the time, the probability (density) of our sample can be written:\[p\left((x_i)_{i=1,\ldots,n};(t_i)_{i=1,\ldots,n}, \theta\right)= \prod_{i=1}^n p(x_i; t_i, \theta)\, .\]
- Our collection of models, $\mathcal{M}(\theta)$, is then made of all the functions $\mathbb{R}^n \mapsto \mathbb{R}$ of the form: \[\prod_{i=1}^n p(X_i; t_i, \theta) \quad \text{with} \quad (t_i)_{i=1,\ldots,n} \ge 0 \quad \text{fixed}\, .\]

** Our problem
- We want to find the member of our collection that "explains best" the data.
- Stated differently, we want to find $\hat{\theta}$ such that $\mathcal{M}(\hat{\theta})$ "explains best" the data.
- If the data are put to use that means that $\hat{\theta}$ will depend on them, *$\hat{\theta}$ must be a function of $(x_i)$*.
- We are going to be optimistic and assume that the data *were actually generated by one member of our collection* and we will write $\theta_0$ the index of this member.
- $\hat{\theta}(x_1,\ldots,x_n)$ is then called *an estimator* of $\theta_0$.
- Finding the "best explanation" amounts then to finding $\hat{\theta}(x_1,\ldots,x_n)$ as close as possible to $\theta_0$. 

** Consequences
- Since all members of our collection are *probability densities*, we are explicitly considering that if we repeat our experiment in the exact same conditions we will get $(y_i)_{1,\ldots,n} \neq (x_i)_{1,\ldots,n}$.
- We therefore expect (in general) that $\hat{\theta}(y_1,\ldots,y_n) \neq \hat{\theta}(x_1,\ldots,x_n)$. 
- Stated differently, *since $\hat{\theta}$ depends on the observations, it is a random variable*.
- The notion of "finding $\hat{\theta}(x_1,\ldots,x_n)$ as close as possible to $\theta_0$" must then be made more precise.
- Doing as if we could perform as many experiments as we wish, we will look for estimators whose mean value $\lim_{n \rightarrow \infty} \mathtt{E}\hat{\theta}(x_i) = \theta_0$. Such estimators are called *asymptotically unbiased*.
- We would also like the distribution of $\hat{\theta}$ to be as concentrated as possible around $\theta_0$, that is, to have a small variance. 
