# -*- ispell-local-dictionary: "american" -*-
#+TITLE: A glimpse of the Statistician's toolbox
#+AUTHOR: @@latex:{\large Christophe Pouzat} \\ \vspace{0.2cm}MAP5, Paris-Descartes University and CNRS\\ \vspace{0.2cm} \texttt{christophe.pouzat@parisdescartes.fr}@@
#+DATE: LASCON, January 22 2018, Lecture 2
#+OPTIONS: H:2 tags:nil
#+EXCLUDE_TAGS: noexport
#+LANGUAGE: en
#+SELECT_TAGS: export
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [presentation]
#+LATEX_HEADER: \usepackage{dsfont}
#+BEAMER_HEADER: \setbeamercovered{invisible}
#+BEAMER_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{Where are we ?}\tableofcontents[currentsection]\end{frame}}
#+BEAMER_HEADER: \beamertemplatenavigationsymbolsempty
#+STARTUP: beamer
#+COLUMNS: %45ITEM %10BEAMER_ENV(Env) %10BEAMER_ACT(Act) %4BEAMER_COL(Col) %8BEAMER_OPT(Opt)
#+STARTUP: indent
#+PROPERTY: header-args :eval no-export

* Setup :noexport:
#+NAME: set-gnuplot-pars
#+BEGIN_SRC gnuplot :session *gnuplot* :results silent :eval no-export :exports none 
set terminal pngcairo size 1000,1000
#+END_SRC

#+NAME: stderr-redirection
#+BEGIN_SRC emacs-lisp :exports none
;; Redirect stderr output to stdout so that it gets printed correctly (found on
;; http://kitchingroup.cheme.cmu.edu/blog/2015/01/04/Redirecting-stderr-in-org-mode-shell-blocks/
(setq org-babel-default-header-args:sh
      '((:prologue . "exec 2>&1") (:epilogue . ":"))
      )
(setq org-babel-use-quick-and-dirty-noweb-expansion t)
#+END_SRC

#+RESULTS: stderr-redirection
: t

* Introduction :export:
** What are we going to talk about? 
- Descriptive statistics that are *robust*: =median=, =median absolute deviation=, =five-number summary=.
- =Cumulative Distribution Functions= (CDF) and their observed or empirical versions (ECDF).
- The =Likelihood= function.
- The =Maximum Likelihood Estimator= (MLE) and its properties.

* Descriptive statistics :export:
** What makes a statistic "robust"?
- Robust statistics are "well behaved" even when something goes wrong.
- We will illustrate what that means with robust versions of the classical /location/ and /scale/ parameters:
  + the *median* instead of the  /mean/,
  + the *median absolute deviation* (MAD) instead of the /standard deviation/.  

** An example
24 determinations of the copper content in the wholemeal floor (in parts per million) sorted in ascending order (example 1.1 of Maronna, Martin & Yohai, /Robust Statistics. Theory and Methods./ 2006 J. Wiley):
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{tabular}{ r r r r r r r r } 
2.20 & 2.20 & 2.40 & 2.40 & 2.50 & 2.70 & 2.80 & 2.90\\
3.03 & 3.03 & 3.10 & 3.37 & 3.40 & 3.40 & 3.40 & 3.50\\
3.60 & 3.70 & 3.70 & 3.70 & 3.70 & 3.77 & 5.28 & \textcolor{orange}{28.95}
\end{tabular}
#+END_EXPORT

#+BEGIN_EXPORT latex
\vspace{0.2cm}
#+END_EXPORT

- The sample /mean/ is 4.28 while the /median/ is 3.38.

#+BEGIN_EXPORT latex
\vspace{0.2cm}
#+END_EXPORT
If we remove the *outlier*, 28.95, we get:
#+BEGIN_EXPORT latex
\vspace{0.2cm}
#+END_EXPORT
- A /mean/ of 3.21 and a /median/ of 3.37.

*** Making the graph 
:PROPERTIES:
:BEAMER_ENV: note
:END:
#+NAME: copper-table
|  2.20 | 0 |
|  2.20 | 1 |
|  2.40 | 0 |
|  2.40 | 1 |
|  2.50 | 0 |
|  2.70 | 0 |
|  2.80 | 0 |
|  2.90 | 0 |
|  3.03 | 0 |
|  3.03 | 1 |
|  3.10 | 0 |
|  3.37 | 0 |
|  3.40 | 0 |
|  3.40 | 1 |
|  3.40 | 2 |
|  3.50 | 0 |
|  3.60 | 0 |
|  3.70 | 0 |
|  3.70 | 1 |
|  3.70 | 2 |
|  3.70 | 3 |
|  3.77 | 0 |
|  5.28 | 0 |
| 28.95 | 0 |

#+NAME: copper-fig
#+HEADERS: :file imgs/copper_fig.png 
#+BEGIN_SRC gnuplot :var data=copper-table :cache no
unset key
unset border
unset label
set zeroaxis lt -1
set ytics (-1,1)
set arrow from 4.28,-0.04 to 4.28,0.0 lc 'red' lw 2
set arrow from 3.21,0.04 to 3.21,0.0 lc 'red' lw 2
set arrow from 3.38,-0.04 to 3.38,0.0 lc 'blue' lw 2
set arrow from 3.37,0.04 to 3.37,0.0 lc 'blue' lw 2
plot [2:7] [-0.05:0.2] data using 1:(0.05*$2) pt 6 ps 2 lc 'black'
#+END_SRC

#+RESULTS: copper-fig
[[file:imgs/copper_fig.png]]


** 
#+ATTR_LATEX: :width 0.8\textwidth
[[./imgs/copper_fig.png]]

#+BEGIN_EXPORT latex
\vspace{0.2cm}
#+END_EXPORT
The data with the outlier out of scale. Bottom: mean (red) and median (blue) computed with the outlier. Top: mean (red) and median (blue) computed /without/ the outlier.

** What is the MAD?

- We have just seen that using a median instead of a mean "stabilizes" the results (understand: make the result look the same when some observations are removed).
- We want to adapt this idea to the statistics characterizing the /scale/ or /spread/ of the data for which the /standard deviation/ (SD) is usually used.
- The SD is moreover obtained from the square root of a mean of *squared differences* (the difference between each individual observation and the sample mean). If one observation is a genuine outlier /it will dominate the estimate/.
** 
- The /Median Absolute Deviation/ addresses both issues.
- It is proportional to the median of the absolute deviations with respect to the median:\[\mathtt{MAD} = \frac{1}{0.67449} \, \mathtt{median}\left(|X_i - \mathtt{median}(X)|\right)\, ,\] where $X=\{X_1,\ldots,X_n\}$ is the sample.
- The division by 0.67449 makes the MAD equal to the SD (on average) when the sample is drawn from a Gaussian.
- For the copper data, the SD is 5.30 with the complete sample and becomes 0.69 when the outlier is removed.
- *For the same sample, the MAD is 0.53 with the complete sample and becomes 0.50 when the outlier is removed*.
 
** 
- *When you work with real data use the median instead of the mean and the MAD instead of the SD* unless you are pretty sure that your sample contains no "pathological" observations.
- We will see that at work on neurophysiological data when we will discuss spike sorting.

** The five-numbers summary
This is a set of statistics that turns out to be very useful to summarize large data set. It is:
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT
- The /minimum/ of the sample.
- The /first quartile/.
- The /median/ (second /quartile/).
- The /third quartile/
- The /maximum/ of the sample.
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT
The /inter quartile range/ (IQR), the difference between the third and first quartile is another robust estimator of the spread of the data.
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT

*When working with large datasets my recommendation is to compute systematically the five-numbers summary and the MAD. These statistics should appear in your lab-book.* 

** The Empirical Cumulative Distribution Function (ECDF) 
- The /Cumulative Distribution Function/ (CDF) of a random variable $X$ is by definition:\[F_X(x) \equiv \mathbb{P}(X \le x) \, ,\] where $\mathbb{P}(X \le x)$ stands for "the probability of the event $X \le x$".  
- Let $X_1,\ldots,X_n \stackrel{\mathrm{IID}}{\sim} F_X$, the /Empirical Cumulative Distribution Function/ (ECDF) of the sample $\{X_1,\ldots,X_n\}$ is (by definition): \[\widehat{\mathrm{F}} \equiv \frac{1}{n} \sum_{i=1}^n \mathds{1}(X_i \le x) \, ,\] where \[\mathds{1}(X_i \le x) = \left\{ \begin{array}{lr} 1 & \mathrm{if}\quad X_i \le x\\ 0 & \mathrm{if}\quad X_i > x.\end{array} \right. \] 

** 
- The ECDF is a function that makes a "jump" of size 1/n at each observation $X_i$.
- If we write $X_{(i)}$ the /order statistics/, that is:
  + $X_{(1)} = \min \{X_1,\ldots,X_n\}$
  + $X_{(2)} = \min \{X_1,\ldots,X_n\} \setminus \{X_{(1)}\}$ 
  + $X_{(k)} = \min \{X_1,\ldots,X_n\} \setminus \{X_{(1)},\ldots,X_{(k-1)}\}$ 
  + $X_{(n)} = \max \{X_1,\ldots,X_n\}$
  the ECDF graph is piecewise constant, continuous on the right side with a limit of the left side (the function's value at a jump site, $X_{(k)}$, is the staircase's height on the right side of $X_{(k)}$).

** An example with historical data 

#+ATTR_LATEX: :width 0.8\textwidth
[[./imgs/FattKatz1952.png]]

In 1952, Fatt and Katz reported the first observation at the frog neuro-muscular junction of *miniature end-plate potentials* (mEPPs).

**  
#+ATTR_LATEX: :width 1.0\textwidth
[[./imgs/FattKatz1952Fig2.png]]

These observations quickly lead to the *quantal release* model of synaptic transmission. Katz got the Nobel Prize for that (Paul Fatt was forgotten...).

** The data

- In their 1952 paper, Fatt and Katz study the time intervals between two successive mEPPs (the results are shown on their Figs. 11 & 12).
- The data can be found in the appendix of a book by David Cox and Peter Lewis (1966) /The Statistical Analysis of Series of Events/ who thank Katz and Miledi for providing the data and who wrongly describe them as measurements between /nerve impulses/ (this is at least what I concluded when I tried to figure out how intervals between nerve impulses--from a single axon--could follow so perfectly a Poisson distribution).
- The data reappear in two recent and excellent books by Larry Wasserman (/All of Statistics/, 2004 and /All of Nonparametric Statistics/, 2006).
- They can be downloaded from L. Wasserman website: [[http://www.stat.cmu.edu/~larry/all-of-nonpar/]].

** 

#+ATTR_LATEX: :width 0.75\textwidth
[[./imgs/Fatt_Katz_ECDF.png]]

ECDF of the inter MEPP times from Fatt and Katz (1952).

** Adding confidence bands

- It is possible and even straightforward to add a *confidence band* to the graph of $\widehat{\mathrm{F}}$.
- A *confidence band* is a domain that contains the *complete graph of the true CDF* with a probability set by us.
- The Kolmogorov distribution can be used, but a simpler to compute distribution--leading to almost as tight bands--results from the *Dvoretzky-Kiefer-Wolfowitz* (DKW) inequality:\[\mathbb{P}\left(\sup_x \mid \mathrm{F}(x) - \widehat{\mathrm{F}}_{n}(x) \mid > \epsilon \right) \le 2\, e^{-2 n \epsilon^2} \, ,\] where $n$ is the sample size.
- So if we want $\epsilon$ such that:\[\mathbb{P}\left(\sup_x \mid \mathrm{F}(x) - \widehat{\mathrm{F}}_{n}(x) \mid > \epsilon \right) \le 1-\alpha\] we find:\[\epsilon(\alpha) = \sqrt{\frac{1}{2 n}} \sqrt{\log\left(\frac{2}{1-\alpha}\right)} \, .\]
 

** 
#+ATTR_LATEX: :width 0.75\textwidth
[[./imgs/Fatt_Katz_ECDF_band.png]]

ECDF with 95% confidence band of the inter MEPP times from Fatt and Katz (1952).

** Why use the ECDF?
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT
- The only "data manipulation" involved is sorting (no bin width setting).
- It is easy to get confidence bands making the ECDF a quantitative tool.
- Histograms lead too easily to baseless conclusions. 

* Models and estimators                                              :export:

** The setting
- We consider a situation where a sample (or a set of observations) $\mathbold{x}=(x_i)_{i=1,\ldots,n}$ is available.
- These observations are modeled as a draw from a probability distribution $\mathcal{M}$, we say that the sample $\mathbold{x}$ is the *realization* of the random variable $\mathbold{X}$ whose distribution is $\mathcal{M}$ ($\mathbold{X} \sim \mathcal{M}$). 
- Our model $\mathcal{M}$ is in fact partly unknown, otherwise we would not need the experiment that gave us $\mathbold{x}$.
- So we really have in mind a collection of models that we write $\mathcal{M}(\theta)$, where $\theta \in \mathbb{R}^p$ and $p < \infty$.

** 
As an example, we could assume:
- The data were generated by measurements along a decaying mono-exponential that we will call "our signal", $s$: \[s(t;b, \Delta, \tau) \equiv b+\Delta \, \exp -t/\tau \, ,\] where $b$ is the baseline, $\Delta$, the jump at zero and $\tau$ the decay time constant.
- These three quantities constitute our model parameter: $\theta \equiv (b, \Delta, \tau)$. 
- The measurements were done at some specific (positive) times $(t_i)_{i=1,\ldots,n}$.
- The measurements were corrupted by an independent Gaussian noise with a know variance $\sigma^2$ and a null mean, leading to the following expression for the /probability density/: \[p(X_i=x; t_i, \theta) = \frac{1}{\sqrt{2\,\pi\,\sigma^2}} \, \exp \left(- \frac{(x-s(t_i;\theta))^2}{2 \sigma^2} \right)\, .\]

** 
- Since we assume that the measurement noise is independent of the signal value and of the time, the probability (density) of our sample can be written:\[p\left((x_i)_{i=1,\ldots,n};(t_i)_{i=1,\ldots,n}, \theta\right)= \prod_{i=1}^n p(x_i; t_i, \theta)\, .\]
- Our collection of models, $\mathcal{M}(\theta)$, is then made of all the functions $\mathbb{R}^n \mapsto \mathbb{R}$ of the form: \[\prod_{i=1}^n p(X_i; t_i, \theta) \quad \text{with} \quad (t_i)_{i=1,\ldots,n} \ge 0 \quad \text{fixed}\, .\]

** Our problem
- We want to find the member of our collection that "explains best" the data.
- Stated differently, we want to find $\hat{\theta}$ such that $\mathcal{M}(\hat{\theta})$ "explains best" the data.
- If the data are put to use that means that $\hat{\theta}$ will depend on them, *$\hat{\theta}$ must be a function of $(x_i)$*.
- We are going to be optimistic and assume that the data *were actually generated by one member of our collection* and we will write $\theta_0$ the index of this member.
- $\hat{\theta}(x_1,\ldots,x_n)$ is then called *an estimator* of $\theta_0$.
- Finding the "best explanation" amounts then to finding $\hat{\theta}(x_1,\ldots,x_n)$ as close as possible to $\theta_0$. 

** Consequences
- Since all members of our collection are *probability densities*, we are explicitly considering that if we repeat our experiment in the exact same conditions we will get $(y_i)_{1,\ldots,n} \neq (x_i)_{1,\ldots,n}$.
- We therefore expect (in general) that $\hat{\theta}(y_1,\ldots,y_n) \neq \hat{\theta}(x_1,\ldots,x_n)$. 
- Stated differently, *since $\hat{\theta}$ depends on the observations, it is a random variable*.
- The notion of "finding $\hat{\theta}(x_1,\ldots,x_n)$ as close as possible to $\theta_0$" must then be made more precise.
- Doing as if we could perform as many experiments as we wish, we will look for estimators whose mean value satisfy $\lim_{n \rightarrow \infty} \mathtt{E}\hat{\theta}(x_i) = \theta_0$. Such estimators are called *asymptotically unbiased* or *consistent*.
** 
- We would also like the distribution of $\hat{\theta}$ to be as concentrated as possible around $\theta_0$, that is, to have a small variance. 
- Comparing the results of two independent experiments means comparing $\hat{\theta}(y_1,\ldots,y_n)$ and $\hat{\theta}(x_1,\ldots,x_n)$ and we would like a /yardstick/ allowing us to judge how different are two estimated values.
- Stated more formally we want a procedure that providence *confidence intervals* on the estimated parameters.
- *Implementing reproducible research is just impossible without confidence intervals* (as soon as the observations are variable at least).

* The likelihood function  :export:
** The likelihood function 
- In the previous section we defined a "model" as a probability (density) function depending on some parameters $\mathcal{M}(\hat{\theta})$ like \[\prod_{i=1}^n p(X_i; t_i, \theta) \quad \text{with} \quad (t_i)_{i=1,\ldots,n} \ge 0 \quad \text{fixed}\, .\]
- Once data have been observed and "plugged-in" the probability density function: \[\prod_{i=1}^n p(x_i; t_i, \theta) \quad \text{with} \quad (t_i,\textcolor{red}{x_i})_{i=1,\ldots,n} \ge 0 \quad \text{fixed}\, ,\] we can view this object as a function of $\theta$.
- *The likelihood function is "just" that: the probability density applied to "fixed" data, viewed as a function of the model parameters, $L(\theta)$.* 
** Variations
- We will not focus on the precise values taken by the likelihood function but on each shape.
- We will therefore (in general) drop the factors that do not depend on the model parameters.
- Since the likelihood function originates from a probability (density), it is positive and we can take its logarithm. Knowing the log-likelihood is like knowing the likelihood.
- For theoretical reasons that will be explained later and for numerical reason (the likelihood can be very very small) we will work with the log-likelihood, $l(\theta) = \log L(\theta)$, most of the time.

** An empirical study  
Let us almost go back to the previous example, the mono-exponential decay, with the following modification:
- The observations are made a regularly separated time points with a time $\delta$ between two successive observations: $(t_i)_{i=1,\ldots,n} = (\delta i)_{i=1,\ldots,n}$.
- The measurement noise depends on the signal $s(t;\theta)$,
- The observation at time $t_i$ is the realization of a Poisson random variable with parameter $s(t_i;\theta)$: \[\mathbb{P}\{X_i = n\} = \frac{(s_i)^n}{n!} \exp (-s_i)\, , \quad \mathrm{for} \quad n=0,1,2,\ldots \] and \[s_i = s(\delta i; \theta) = b + \Delta \, \exp (- \delta i / \tau)\; .\]
#+BEGIN_EXPORT latex
\vspace{0.2cm}
#+END_EXPORT

*This simple scheme contains all the key ingredients to work with fluorescence measurements.*  

** 
#+ATTR_LATEX: :width 0.8\textwidth
[[./imgs/exp-relaxation-illustration1.png]]

Example of simulated data with $b=10$, $\Delta=90$, $\tau=1$.

** 
- In order to have simpler displays, we will start with a setting where both parameters $b$ and $\Delta$ are known.
- The log-likelihood is then a function of a single variable $\tau$. 
- We will also do as if only two times add been used, $t_1$ and $t_2$ leading to observations $x_1$ and $x_2$ on the previous figure.
- The log-likelihood is then: \[l(\tau) = x_1\, \log s(t_1,\tau) - s(t_1,\tau) + x_2\, \log s(t_2,\tau) - s(t_2,\tau) \, .\]
- To make comparison with subsequent simulations in the same setting we will show the graph of: \[r(\tau) = l(\hat{\tau}) - l(\tau) \, ,\] where $\hat{\tau}$ is the location of the maximum of $l(\tau)$.

**   
#+ATTR_LATEX: :width 0.8\textwidth
[[./imgs/exp-relaxation-illustration1-r.png]]
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT

On the next slide we repeat what we just did with 9 additional samples.

** 
#+ATTR_LATEX: :width 0.8\textwidth
[[./imgs/exp-relaxation-illustration1-10-r.png]]
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT

On the next slide we repeat what we just did but we now use 4 times more observations (that is 8) per sample.

** 
#+ATTR_LATEX: :width 0.8\textwidth
[[./imgs/exp-relaxation-illustration2-r.png]]
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT

What do you see?

** What you should have seen
As the sample size increase:
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT
- The log-likelihood becomes more symmetric.
- The curvature increases.
- The location of the peak gets closer to the true value.

* The Maximum Likelihood Estimator :export:

** The MLE
- In 1922 Fischer proposed to take as an estimator $\hat{\theta}$ the $\theta$ that maximizes $l(\theta)$.
- In that he was essentially following and generalizing Daniel Bernoulli, Lambert and Gauss.
- But he went much farther claiming that when the maximum was a smooth maximum, obtained by taking the derivative / gradient with respect to $\theta$ and setting it equal to 0, then:
  + The accuracy (standard error of the estimate) can be found to a good approximation from the curvature of $l(\theta)$ at its maximum.
  + $\hat{\theta}$ expresses all the relevant information available in the data.
  + This estimate is the best of all the consistent ones. 

** Some remarks
- The MLE is just the value of the parameter that makes the observations most probable /a posteriori/.
- Some technical precautions are required in order to fulfill all of Fischer's promises; they are referred to as "the appropriate smoothness conditions" in the literature.  
- They are heavy to state and a real pain to check (that's why no one checks them)!
- My recommendation is to go ahead and after the MLE, $\hat{\theta}$, has been found, do a parametric bootstrap:
  + take $\hat{\theta}$ has the "true" value and simulate 500 to 1000 samples from $\mathcal{M}(\hat{\theta})$,
  + for each simulated sample, repeat the estimation procedure and get a sample of 500 to 1000 $\hat{\theta}$ values,
  + check that this sample has the properties expected from MLE theory.
- With this bootstrap procedure we also make sure that the "asymptotic" regime is reached (the theorems are valid when the sample size goes to infinity).

** Illustration: comparison of two estimators
We reconsider our first simulation, 2 observations, $b$ and $\Delta$ known and we compare two estimators for $\tau$:
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT 
- The MLE, $\hat{\tau}$, that maximizes: \[l(\tau) = x_1\, \log s(t_1,\tau) - s(t_1,\tau) + x_2\, \log s(t_2,\tau) - s(t_2,\tau) \, .\]
- The least squares estimator, $\tilde{\tau}$, that minimizes: \[rss(\tau) = (x_1-s(t_1,\tau))^2+(x_2-s(t_2,\tau))^2\, .\]
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT 

We simulate 10000 samples and get both estimators for each.

** 
#+ATTR_LATEX: :width 0.8\textwidth
[[./imgs/exp-relaxation-estimator-comparison.png]]
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT

$\tilde{\tau}$ is blue and $\hat{\tau}$ is orange.

** Some remarks

- Maximum likelihood estimation is very general: as soon as we have an expression for the probability (density) of our data, we can use it.
- It assumes that the true model is in the family we consider, *this is a very strong assumption*.
- We must therefore always do some goodness of fit test, otherwise the confidence intervals we will get will be meaningless.
- Doing maximum likelihood means that we do optimization all the time. The users of this method should make a minimal effort to master the numerical optimization routines they are going to use.
- Be careful with the optimization routines of =Scipy=.  
