# -*- ispell-local-dictionary: "american" -*-
#+TITLE: A glimpse of the Statistician's toolbox
#+AUTHOR: @@latex:{\large Christophe Pouzat} \\ \vspace{0.2cm}MAP5, Paris-Descartes University and CNRS\\ \vspace{0.2cm} \texttt{christophe.pouzat@parisdescartes.fr}@@
#+DATE: LASCON, January 22 2018, Lecture 2
#+OPTIONS: H:2 tags:nil
#+EXCLUDE_TAGS: noexport
#+LANGUAGE: en
#+SELECT_TAGS: export
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [presentation]
#+BEAMER_HEADER: \setbeamercovered{invisible}
#+BEAMER_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{Where are we ?}\tableofcontents[currentsection]\end{frame}}
#+BEAMER_HEADER: \beamertemplatenavigationsymbolsempty
#+STARTUP: beamer
#+COLUMNS: %45ITEM %10BEAMER_ENV(Env) %10BEAMER_ACT(Act) %4BEAMER_COL(Col) %8BEAMER_OPT(Opt)
#+STARTUP: indent
#+PROPERTY: header-args :eval no-export

* Setup :noexport:
#+NAME: set-gnuplot-pars
#+BEGIN_SRC gnuplot :session *gnuplot* :results silent :eval no-export :exports none 
set terminal pngcairo size 1000,1000
#+END_SRC

* Introduction :export:
** What are we going to talk about? 
- Descriptive statistics that are *robust*: =median=, =median absolute deviation=, =five-number summary=.
- =Cumulative Distribution Functions= (CDF) and their observed or empirical versions (ECDF).
- =Histograms= and the choice of bin width in histograms.
- The =Likelihood= function.
- The =Maximum Likelihood Estimator= (MLE) and its properties.

* Descriptive statistics :export:
** What makes a statistic "robust"?
- Robust statistics are "well behaved" even when something goes wrong.
- We will illustrate what that means with robust versions of the classical /location/ and /scale/ parameters:
  + the *median* instead of the  /mean/,
  + the *median absolute deviation* (MAD) instead of the /standard deviation/.  

** An example
24 determinations of the copper content in the wholemeal floor (in parts per million) sorted in ascending order (example 1.1 of Maronna, Martin & Yohai, /Robust Statistics. Theory and Methods./ 2006 J. Wiley):
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{tabular}{ r r r r r r r r } 
2.20 & 2.20 & 2.40 & 2.40 & 2.50 & 2.70 & 2.80 & 2.90\\
3.03 & 3.03 & 3.10 & 3.37 & 3.40 & 3.40 & 3.40 & 3.50\\
3.60 & 3.70 & 3.70 & 3.70 & 3.70 & 3.77 & 5.28 & \textcolor{orange}{28.95}
\end{tabular}
#+END_EXPORT

#+BEGIN_EXPORT latex
\vspace{0.2cm}
#+END_EXPORT

- The sample /mean/ is 4.28 while the /median/ is 3.38.

#+BEGIN_EXPORT latex
\vspace{0.2cm}
#+END_EXPORT
If we remove the *outlier*, 28.95, we get:
#+BEGIN_EXPORT latex
\vspace{0.2cm}
#+END_EXPORT
- A /mean/ of 3.21 and a /median/ of 3.37.

*** Making the graph 
:PROPERTIES:
:BEAMER_ENV: note
:END:
#+NAME: copper-table
|  2.20 | 0 |
|  2.20 | 1 |
|  2.40 | 0 |
|  2.40 | 1 |
|  2.50 | 0 |
|  2.70 | 0 |
|  2.80 | 0 |
|  2.90 | 0 |
|  3.03 | 0 |
|  3.03 | 1 |
|  3.10 | 0 |
|  3.37 | 0 |
|  3.40 | 0 |
|  3.40 | 1 |
|  3.40 | 2 |
|  3.50 | 0 |
|  3.60 | 0 |
|  3.70 | 0 |
|  3.70 | 1 |
|  3.70 | 2 |
|  3.70 | 3 |
|  3.77 | 0 |
|  5.28 | 0 |
| 28.95 | 0 |

#+NAME: copper-fig
#+HEADERS: :file imgs/copper_fig.png 
#+BEGIN_SRC gnuplot :exports both :var data=copper-table :cache no
unset key
unset border
unset label
set zeroaxis lt -1
set ytics (-1,1)
set arrow from 4.28,-0.04 to 4.28,0.0 lc 'red' lw 2
set arrow from 3.21,0.04 to 3.21,0.0 lc 'red' lw 2
set arrow from 3.38,-0.04 to 3.38,0.0 lc 'blue' lw 2
set arrow from 3.37,0.04 to 3.37,0.0 lc 'blue' lw 2
plot [2:7] [-0.05:0.2] data using 1:(0.05*$2) pt 6 ps 2 lc 'black'
#+END_SRC

#+RESULTS: copper-fig
[[file:imgs/copper_fig.png]]


** 
#+ATTR_LATEX: :width 0.8\textwidth
[[./imgs/copper_fig.png]]

#+BEGIN_EXPORT latex
\vspace{0.2cm}
#+END_EXPORT
The data with the outlier out of scale. Bottom: mean (red) and median (blue) computed with the outlier. Top: mean (red) and median (blue) computed /without/ the outlier.

** What is the MAD?

- We have just seen that using a median instead of a mean "stabilizes" the results (understand: make the result look the same when some observations are removed).
- We want to adapt this idea to the statistics characterizing the /scale/ or /spread/ of the data for which the /standard deviation/ (SD) is usually used.
- The SD is moreover obtained from the square root of a mean of *squared differences* (the difference between each individual observation and the sample mean). If one observation is a genuine outlier /it will dominate the estimate/.
** 
- The /Median Absolute Deviation/ addresses both issues.
- It is proportional to the median of the absolute deviations with respect to the median:\[\mathtt{MAD} = \frac{1}{0.67449} \, \mathtt{median}\left(|X_i - \mathtt{median}(X)|\right)\, ,\] where $X=\{X_1,\ldots,X_n\}$ is the sample.
- The division by 0.67449 makes the MAD equal to the SD (on average) when the sample is drawn from a Gaussian.
- For the copper data, the SD is 5.30 with the complete sample and becomes 0.69 when the outlier is removed.
- *For the same sample, the MAD is 0.53 with the complete sample and becomes 0.50 when the outlier is removed*.
 
** 
- *When you work with real data use the median instead of the mean and the MAD instead of the SD* unless you are pretty sure that your sample contains no "pathological" observations.
- We will see that at work on neurophysiological data when we will discuss spike sorting.

** The five-numbers summary
This is a set of statistics that turns out to be very useful to summarize large data set. It is:
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT
- The /minimum/ of the sample.
- The /first quartile/.
- The /median/ (second /quartile/).
- The /third quartile/
- The /maximum/ of the sample.
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT
The /inter quartile range/ (IQR), the difference between the third and first quartile is another robust estimator of the spread of the data.
#+BEGIN_EXPORT latex
\vspace{0.25cm}
#+END_EXPORT

*When working with large datasets my recommendation is to compute systematically the five-numbers summary and the MAD. That should appear in your lab-book.* 
